# NaiveBayes_KDE.py
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, confusion_matrix
from sklearn.neighbors import KernelDensity
from sklearn.preprocessing import StandardScaler
import joblib

class NaiveBayesKDE:
    """
    Naive Bayes classifier where p(x_j | y) is estimated via KDE per feature j and class y.
    - kde_models is a dict: kde_models[class][feature_index] -> KernelDensity fitted
    - priors: class prior (empirical or smoothed)
    """
    def __init__(self, bandwidth=1.0, kernel='gaussian', bandwidths_per_feature=False):
        self.bandwidth = bandwidth
        self.kernel = kernel
        self.bandwidths_per_feature = bandwidths_per_feature
        self.kde_models = {}
        self.classes_ = None
        self.priors_ = {}
        self.feature_scaler = None  # optional scaler used before fitting KDEs

    def fit(self, X, y, scaler=None, bandwidth_grid=None):
        """
        X: numpy array shape (n_samples, n_features)
        y: array-like of labels
        scaler: optional sklearn scaler already fitted or None (if provided, we will use it)
        bandwidth_grid: If provided and bandwidths_per_feature==True, expects list/array to try in cross-validation.
        """
        X = np.asarray(X)
        y = np.asarray(y)
        n, d = X.shape
        self.classes_, counts = np.unique(y, return_counts=True)
        total = n
        # Priors (empirical)
        self.priors_ = {c: (counts[i] / total) for i, c in enumerate(self.classes_)}

        # Optional scaler (helps KDE when features have different scales)
        if scaler is not None:
            self.feature_scaler = scaler
            X_scaled = scaler.transform(X)
        else:
            X_scaled = X

        # Fit KDE per class and per feature
        for i, c in enumerate(self.classes_):
            Xc = X_scaled[y == c]
            self.kde_models[c] = {}
            for j in range(d):
                col = Xc[:, j].reshape(-1, 1)
                bw = self.bandwidth
                # if bandwidths_per_feature: you'd implement cross-val per feature here
                kde = KernelDensity(kernel=self.kernel, bandwidth=bw)
                kde.fit(col)
                self.kde_models[c][j] = kde
        return self

    def _log_likelihood(self, X):
        """
        Return log p(x | class) for each class and each sample.
        Output: dict[class] = array shape (n_samples,)
        """
        X = np.asarray(X)
        if self.feature_scaler is not None:
            Xs = self.feature_scaler.transform(X)
        else:
            Xs = X
        n, d = Xs.shape
        log_lik = {}
        for c in self.classes_:
            # sum log densities across features (Naive Bayes independence)
            s = np.zeros(n)
            for j in range(d):
                vals = Xs[:, j].reshape(-1, 1)
                kde = self.kde_models[c][j]
                log_dens = kde.score_samples(vals)  # log p(x_j | c)
                s += log_dens
            log_lik[c] = s
        return log_lik

    def predict_proba(self, X):
        """
        Returns probability for each class per sample in shape (n_samples, n_classes),
        ordered by self.classes_.
        """
        X = np.asarray(X)
        n = X.shape[0]
        log_prior = {c: np.log(self.priors_[c]) for c in self.classes_}
        log_lik = self._log_likelihood(X)  # dict[class] -> (n,)
        # compute unnormalized log posterior
        log_post = np.zeros((n, len(self.classes_)))
        for idx, c in enumerate(self.classes_):
            log_post[:, idx] = log_prior[c] + log_lik[c]
        # Normalize with log-sum-exp for numeric stability
        max_log = np.max(log_post, axis=1, keepdims=True)
        stabilized = log_post - max_log
        post = np.exp(stabilized)
        post_sum = np.sum(post, axis=1, keepdims=True)
        probs = post / post_sum
        return probs  # shape (n, n_classes)

    def predict(self, X):
        probs = self.predict_proba(X)
        idx = np.argmax(probs, axis=1)
        return self.classes_[idx]

    def save(self, path):
        joblib.dump({
            'kde_models': self.kde_models,
            'classes_': self.classes_,
            'priors_': self.priors_,
            'bandwidth': self.bandwidth,
            'kernel': self.kernel,
            'feature_scaler': self.feature_scaler
        }, path)

    @classmethod
    def load(cls, path):
        data = joblib.load(path)
        inst = cls(bandwidth=data['bandwidth'], kernel=data['kernel'])
        inst.kde_models = data['kde_models']
        inst.classes_ = data['classes_']
        inst.priors_ = data['priors_']
        inst.feature_scaler = data.get('feature_scaler', None)
        return inst

# ---------------------------
# Example usage (end-to-end)
# ---------------------------
if __name__ == "__main__":
    # --- ejemplo sintÃ©tico ---
    from sklearn.datasets import make_classification
    X, y = make_classification(n_samples=2000, n_features=6, n_informative=4,
                               n_redundant=0, n_clusters_per_class=2, weights=[0.9, 0.1],
                               random_state=42)

    # Split
    from sklearn.model_selection import train_test_split
    Xtr, Xt, ytr, yt = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

    # Optional scaler (improves KDE if feature scales vary)
    scaler = StandardScaler().fit(Xtr)

    # Grid-search bandwidth by maximizing CV log-likelihood or AUC
    # (simple approach: try a few bandwidths and pick best by CV AUC)
    candidate_bws = [0.1, 0.3, 0.6, 1.0, 2.0]
    best_bw = None
    best_score = -np.inf
    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    for bw in candidate_bws:
        scores = []
        for train_idx, val_idx in skf.split(Xtr, ytr):
            model = NaiveBayesKDE(bandwidth=bw)
            model.fit(Xtr[train_idx], ytr[train_idx], scaler=scaler)
            probs = model.predict_proba(Xtr[val_idx])  # shape (n_val, n_classes)
            # choose prob for positive class (assume classes_[1] is positive)
            pos_idx = list(model.classes_).index(1)
            auc = roc_auc_score(ytr[val_idx], probs[:, pos_idx])
            scores.append(auc)
        mean_auc = np.mean(scores)
        if mean_auc > best_score:
            best_score = mean_auc
            best_bw = bw
    print("Mejor bandwidth por CV AUC:", best_bw, "AUC:", best_score)

    # Fit final model
    model = NaiveBayesKDE(bandwidth=best_bw)
    model.fit(Xtr, ytr, scaler=scaler)

    # Evaluate
    probs = model.predict_proba(Xt)
    pos_idx = list(model.classes_).index(1)
    yhat = model.predict(Xt)
    auc = roc_auc_score(yt, probs[:, pos_idx])
    precision, recall, f1, _ = precision_recall_fscore_support(yt, yhat, average='binary')
    cm = confusion_matrix(yt, yhat)
    print("Test AUC:", auc)
    print("Precision, Recall, F1:", precision, recall, f1)
    print("Confusion matrix:\n", cm)

    # Save model
    model.save('nb_kde_model.joblib')
